{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"colab":{"name":"TP1_Numpy.ipynb","provenance":[],"collapsed_sections":["5l_2xlEhJJqV","xS8HX2ZRJJqZ","46bVwPbBJJqb","2gs9BMiUJJqb","vr4GNxUQJJqe","BcexWt_MJJqf","bsZ8tQ2_JJqg","jiu9e_hRJJqh","A4viXSG3JJqj","zB0VnW4VJJqv","i5mLy32JJJqw"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2XRltpLfJJqG"},"source":["# Lab 1 -  Scientific computing in Python with Numpy\n","\n","This tutorial provides a brief introduction to scientific computing in Python with [Numpy](www.numpy.org). This package contains (among other things):\n"," - a powerful N-dimensional array,\n"," - sophisticated array operations,\n"," - tools for integrating C/C++ and Fortran code,\n"," - useful linear algebra, Fourier transform, and random number capabilities."]},{"cell_type":"markdown","metadata":{"id":"0LE6QPKXJJqN"},"source":["One good reason to use Numpy is the computational efficiency. For example, suppose you want to compute the exponential of a list of numbers. An option would be to use the function `math.exp()`. Unfortunately, the latter doesn't work on lists!"]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"5tR_x5VpJJqN","executionInfo":{"status":"error","timestamp":1644416066194,"user_tz":-60,"elapsed":33,"user":{"displayName":"Yamna OUCHTAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01529658261698667727"}},"outputId":"9363049c-2437-4250-9c89-e71057766d42"},"source":["import math\n","\n","x = [1, 2, 3]\n","\n","math.exp(x) # you will see this give an error when you run it, because x is a list."],"execution_count":1,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-57a2f655a534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you will see this give an error when you run it, because x is a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: must be real number, not list"]}]},{"cell_type":"markdown","metadata":{"id":"6OsNDcdaJJqO"},"source":["In machine learning, we always organize our data in vectors and matrices. That's why Numpy is more useful than the \"math\" library. In fact, if $x = (x_1, x_2, ..., x_N)$ is a vector, then the function `numpy.exp(x)` will apply the exponential to every element of `x`, and the output will be $e^x = (e^{x_1}, e^{x_2}, ..., e^{x_N})$. The advantange of doing so is that *for*-loops and *while*-loops can be entirely replaced with fast operations on vectors and matrices!"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_GlKLdGGJJqO"},"source":["import numpy as np\n","\n","x = np.array([1, 2, 3])\n","\n","print(np.exp(x)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8sLaYaHJJqP"},"source":["Any time you need more info on a numpy function, we encourage you to look at [the official documentation](https://docs.scipy.org/doc/numpy/reference/index.html). You can also create a new cell in the notebook and write `np.exp?` (for example) to get quick access to the documentation. Now, let's get started!\n","\n","---\n","Before reading this tutorial you should know a bit of Python. If you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"baqdvigIJJqP"},"source":["## 1. Tensors\n","\n","A Numpy tensor is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers. Its class is called `numpy.array`. The more important attributes of a Numpy array are:\n","\n"," - `ndim` - The number of axes of the array.\n"," \n"," - `shape` - A tuple of integers indicating the size of the array in each axis.\n"," \n"," - `size` - The total number of elements of the array (equal to the product of the elements of `shape`).\n","\n"," - `dtype` - The type of the elements in the array (e.g., `numpy.int32`, `numpy.float64`, etc)."]},{"cell_type":"markdown","metadata":{"id":"HwyJliv0JJqQ"},"source":["### Scalars (0D tensors)\n","\n","A tensor that contains only one number is called a scalar. In Numpy, a `float32` or `float64` number is a scalar tensor. Here’s a Numpy scalar:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bomwsiRuJJqQ"},"source":["x = np.array(12)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stTXOxQTJJqR"},"source":["A scalar has **no axis**."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"TnydgxkEJJqR"},"source":["x.ndim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"i2RpXQrAJJqS"},"source":["x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0EGGBXxJJqS"},"source":["### Vectors (1D tensors)\n","\n","An array of numbers is called a vector (or 1D tensor). Following is a Numpy vector:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"PxDoOHHSJJqS"},"source":["a = np.array([0, 1, 8, 27, 64, 125, 216, 343, 512, 729])\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpJKpRgOJJqT"},"source":["A vector has **one axis**. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"GMyBuN9zJJqT"},"source":["a.ndim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kHrrPmT5JJqT"},"source":["This particular vector has ten entries, so its size is 10 and its shape is (10,)."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"kKjfKRhnJJqT"},"source":["a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7Z_HCEnJJqU"},"source":["Vectors can be indexed, sliced and iterated over, much like lists and other Python sequences."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"RrOUXtmRJJqU"},"source":["# Access to the element in position 2\n","a[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"gTEeJO5jJJqU"},"source":["# Access to elements from position 2 to position 4\n","a[2:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5SWRS_rbJJqV"},"source":["# Modify elements from start to position 5, every 2nd element (Sintax: [start:stop:step])\n","a[:6:2] = -100    \n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"pz8FCimjJJqV"},"source":["# Reverse a\n","a[::-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"tWS7I6q4JJqV"},"source":["# Iterate over the elements of a\n","for i in a:\n","    print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l_2xlEhJJqV"},"source":["### Matrices (2D tensors)\n","\n","An array of vectors is called a matrix (or 2D tensor).  This is a Numpy matrix:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"SNH0Y_1aJJqW"},"source":["b = np.array([[5, 78, 2, 34, 0],\n","              [6, 79, 3, 35, 1],\n","              [7, 80, 4, 36, 2],\n","              [8, 81, 5, 37, 3]])\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"byQoGTRMJJqW"},"source":["A matrix has **two axes**. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Zy4OUpHcJJqW"},"source":["b.ndim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0u7nD1tmJJqW"},"source":["This specific matrix has four-by-five entries. Hence, its size is 20 and its shape is (4,5)."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"TeULZzL-JJqW"},"source":["b.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TyaeHLrdJJqX"},"source":["Note that:\n"," - You can visually interpret a matrix as a rectangular grid of numbers. \n"," - The entries from the first axis (axis=0) are called **rows**\n"," - The entries from the second axis (axis=1) are called **columns**.\n"," - Matrix elements can be accessed with two indices, given in a tuple separated by commas."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_adSCCQUJJqX"},"source":["# Acces to the element in position (2,3)\n","b[2,3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ZV4hXI5_JJqX"},"source":["# Access to the 2nd column\n","b[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7xJR_yX5JJqX"},"source":["# Access to 2nd and 3rd rows\n","b[1:3,:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jeB7qhjJJqX"},"source":["When fewer indices are provided than the number of axes, the missing indices are considered complete slices: e.g., `b[i]` is treated as `b[i,:]`."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"r5CVcvwVJJqY"},"source":["# Access the last row\n","b[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1ZVk0fUJJqY"},"source":["The `None` object can be used in all slicing operations to create an axis of length one."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"xNfY8YYWJJqY"},"source":["c = b[-1] # This is a vector\n","c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"geca72BuJJqY"},"source":["c = b[-1,None] # This is a matrix with one row\n","c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"nIJfc3OUJJqY"},"source":["c = b[:,[-1]] # This is a matrix with one column\n","c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lU_tAIoqJJqY"},"source":["Iterating over matrices is done with respect to the rows:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"QCfvYqXpJJqZ"},"source":["for row in b:\n","    print(row)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xS8HX2ZRJJqZ"},"source":["### ND tensors\n","\n","If you pack matrices in a new array, you obtain a 3D tensor. By packing 3D tensors in an array, you can create a 4D tensor, and so on. \n","\n","Following is a Numpy 3D tensor:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"te9ChcwgJJqZ"},"source":["c = np.array([[[5, 78, 2, 34, 0],\n","               [6, 79, 3, 35, 1],\n","               [7, 80, 4, 36, 2],\n","               [8, 81, 5, 37, 3]],\n","              \n","              [[78, 2, 34, 0, 5],\n","               [79, 3, 35, 1, 6],\n","               [80, 4, 36, 2, 7],\n","               [81, 5, 37, 3, 8]],\n","              \n","              [[0, 5, 78, 2, 34],\n","               [1, 6, 79, 3, 35],\n","               [2, 7, 80, 4, 36],\n","               [3, 8, 81, 5, 37]]])\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-nHGlJQJJqZ"},"source":["A ND tensor has **3 or more** axes."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"unZS-TgkJJqZ"},"source":["c.ndim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zhYUCnBJJqZ"},"source":["This tensor has three-by-four-by-five entries. Hence, its size is 60 and its shape is (3,4,5)."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"d2K-ksnLJJqa"},"source":["c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYTJ88CRJJqa"},"source":["Tensor elements can be accessed by giving one index per axis. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"38ho6MbcJJqa"},"source":["# Acces to the element in position (1,3,2)\n","c[1,3,2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijUETuYmJJqa"},"source":["Iterating over multidimensional arrays is done with respect to the first axis:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"DL1qrTtVJJqa"},"source":["for matrix in c:\n","    print(matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46bVwPbBJJqb"},"source":["---\n","#### Exercise - Array creation and manipulation\n","\n","This is a collection of short exercises that will help you understand the basics of Numpy arrays. Solve them!\n","\n","1. Create a vector of size 10, with all zeros but the fifth value which is 1. *Hint:* `numpy.zeros()`.\n","```\n","Expected output: [0 0 0 0 1 0 0 0 0 0]\n","```\n","\n","- Create a vector with values ranging from 10 to 19. *Hint:* `numpy.arange()`.\n","```\n","Expected output: [10 11 12 13 14 15 16 17 18 19]\n","```\n","\n","- Create a 8x8 matrix and fill it with a checkerboard pattern.\n","```\n","Expected output: [[0 1 0 1 0 1 0 1]\n","                    [1 0 1 0 1 0 1 0]\n","                    [0 1 0 1 0 1 0 1]\n","                    [1 0 1 0 1 0 1 0]\n","                    [0 1 0 1 0 1 0 1]\n","                    [1 0 1 0 1 0 1 0]\n","                    [0 1 0 1 0 1 0 1]\n","                    [1 0 1 0 1 0 1 0]]\n","```"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"EJtJBByZJJqb"},"source":["### ADD CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gs9BMiUJJqb"},"source":["### Reshaping\n","\n","An array has a shape given by the number of elements along each axis:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"0uFUH3zHJJqb"},"source":["a = np.floor(10*np.random.random((3,4)))\n","\n","print(\"Shape: \" + str(a.shape))\n","print()\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kegZZejAJJqb"},"source":["The shape of an array can be changed with various commands, **without** changing the elements in the original array."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Z_U0z4gqJJqc"},"source":["# Returns the array flattened as a vector\n","b = a.ravel()\n","\n","print(\"Shape: \" + str(b.shape))\n","print()\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"hVvp6MC-JJqc"},"source":["# Returns the array with a modified shape\n","b = a.reshape(6,2)\n","\n","print(\"Shape: \" + str(b.shape))\n","print()\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"fwObpNStJJqc"},"source":["# Returns the array, transposed\n","b = a.T  \n","\n","print(\"Shape: \" + str(b.shape))\n","print()\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7OYZ6-8MJJqc"},"source":["If a dimension is given as **-1** in a reshaping operation, the other dimensions are automatically calculated."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"zEyKnzoxJJqc"},"source":["b = a.reshape(4,-1)\n","\n","print(\"Shape: \" + str(b.shape))\n","print()\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZioFsxOBJJqc"},"source":["The `np.reshape` function returns its argument with a modified shape, whereas the `np.array.resize` method modifies the array itself:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7CEfiSHVJJqd"},"source":["print(\"Shape: \" + str(a.shape))\n","print()\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"irhhk3Z_JJqd"},"source":["a.resize((2,6))\n","\n","print(\"Shape: \" + str(a.shape))\n","print()\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tboIIlTOJJqd"},"source":["Note that:\n","\n"," - The order of the elements resulting from `ravel()` is “C-style”, in which the **rightmost index** changes the fastest: `a[0,0]`, then `a[0,1]`, and so on. \n"," - If the array is reshaped (or resized) to some other shape, again the array is treated as “C-style”. \n"," - The functions `ravel()`, `reshape()` and `resize()` can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the **leftmost index** changes the fastest: `a[0,0]`, then `a[1,0]`, and so on."]},{"cell_type":"markdown","metadata":{"id":"vr4GNxUQJJqe"},"source":["---\n","#### Exercise - Preparing the training data\n","\n","In computer science, an image is represented by a 3D array of shape `(height, width, depth)`. However, when you use an image as the input of a machine learning algorithm, you usually convert it to a 4D tensor of shape `(1, depth, height, width)`.\n","\n","Here is a pictorial view 4D tensor of shape (samples, Color channels, Height, Width). It is hard to represent on a screen a 4D object, so keep in mind this is just a reprensentation that helps you to understand what you are doing.\n","\n","![4D tensor](https://images4.programmersought.com/753/dc/dc45c040e78f0252e8cd8645be259419.png)\n","\n","**Instructions:** Implement a function `image2tensor()` that takes a tensor of shape `(height, width, depth)` and returns a tensor of shape `(1, depth, height, width)`. Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape`. \n","\n","**Hints:**\n"," - [`np.transpose()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html)\n"," - [`np.reshape()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"aef5092bcd29e7dc9ab7122e11b39981","grade":false,"grade_id":"cell-e4273ad6b5d94df0","locked":false,"schema_version":1,"solution":true},"id":"1LBSbhHMJJqe"},"source":["def image2tensor(image):\n","    \"\"\"\n","    Argument:\n","    image -- a numpy array of shape (height, width, depth)\n","    \n","    Returns:\n","    v -- a vector of shape (1, depth, height, width)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    v = None\n","    ### END CODE HERE ###\n","    \n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"MLG3UbJKJJqf"},"source":["image = np.stack( [np.arange(12).reshape(3,4), np.arange(50,62).reshape(3,4)], axis=2 )\n","\n","print(\"Shape: \" + str(image.shape))\n","print()\n","print(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"-swQDg2OJJqf"},"source":["v = image2tensor(image)\n","\n","print(\"Shape: \" + str(v.shape))\n","print()\n","print(v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlGPejRXJJqf"},"source":["**Expected Output**: \n","\n","<table style = \"width:40%\">\n","    <tr>\n","    <td>** Shape **</td> \n","        <td>(1,2,3,4)</td> \n","    </tr>\n","\n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"23ddbd7f47b648754642ea4d54da81ad","grade":true,"grade_id":"cell-51d959afcfb7eb1b","locked":true,"points":1,"schema_version":1,"solution":false},"id":"kMj9hmnzJJqf"},"source":["assert v.shape == (1,2,3,4)\n","assert np.all(v[0,0] == image[:,:,0])\n","assert np.all(v[0,1] == image[:,:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcexWt_MJJqf"},"source":["### Copies & Views\n","\n","When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:\n"," 1. No Copy at All\n"," - Shallow copy (view)\n"," - Deep copy"]},{"cell_type":"markdown","metadata":{"id":"bsZ8tQ2_JJqg"},"source":["#### No copy at all\n","\n","Simple assignments make no copy of array objects or of their data."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"6KMTr9cjJJqg"},"source":["a = np.arange(12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_ZubpFU8JJqg"},"source":["b = a # no new object is created"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"M_WiRXPlJJqg"},"source":["b is a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Se9gMddaJJqg"},"source":["Python passes mutable objects as references, so function calls make no copy."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"r_6Dyi2jJJqh"},"source":["def f(x):\n","    print(id(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"JpcezAHwJJqh"},"source":["id(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yel_duKTJJqh"},"source":["f(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiu9e_hRJJqh"},"source":["#### Shallow copy\n","\n","Different array objects can share the same data. The view method creates a new array object that looks at the same data."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"k-d2SGV3JJqh"},"source":["a = np.array([[i for i in np.arange(j,j+4)] for j in np.arange(0,12,4)])\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"tTDZSnQiJJqh"},"source":["c = a.view()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"vHYOVE_FJJqh"},"source":["c is a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"pEeau5SqJJqi"},"source":["c.base is a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Qa5dCTr1JJqi"},"source":["c.resize(2,6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"QAoyCryvJJqi"},"source":["a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"G2XSNV-lJJqi"},"source":["c[1,2] = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"IfrFPQSFJJqi"},"source":["print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19suU22uJJqi"},"source":["Slicing an array returns a view of it:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"zX_O-RQ3JJqj"},"source":["s = a[:,1:3]\n","s[:] = 10 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"cINlbOQaJJqj"},"source":["print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4viXSG3JJqj"},"source":["#### Deep copy\n","\n","The `copy()` method makes a complete copy of the array and its data."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3zZ2A4nXJJqj"},"source":["d = a.copy()  # a new array object with new data is created"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7-vkRddaJJqj"},"source":["d is a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"KIR0D0HyJJqj"},"source":["d.base is a   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"YOkiWq4kJJqk"},"source":["d[0,0] = 9999"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"nXpuU-zkJJqk"},"source":["print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Je4JfGcrJJqk"},"source":["print(d)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsVivSC8JJqk"},"source":["## 2. Tensor operations"]},{"cell_type":"markdown","metadata":{"id":"79PMd-yDJJqk"},"source":["### Element-wise operations\n","\n","Arithmetic operations on arrays operate **element-by-element**. A new array is created and filled with the result."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"CQMrHBL6JJqk"},"source":["a = np.array( [20,30,40,50] )\n","b = np.array( [1,2,3,4] )\n","\n","# Arithmetic\n","c = a-b\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmDdWcgtJJql"},"source":["Unlike in many matrix languages, the product operator `*` operates **element-wise** in NumPy arrays, and does not correspond to matrix multiplication. There are special functions for linear algebra that we will cover later."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"-roeWQ4_JJql"},"source":["A = np.array( [[1,1,1],\n","               [0,1,0]] )\n","B = np.array( [[2,0,5],\n","               [3,4,1]] )\n","C = A*B\n","C"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDJiMNAbJJql"},"source":["In binary operations, the two arrays should be the same size. Errors are thrown if arrays do not match in size."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lSdWtVuTJJql"},"source":["x = np.array([1,2,3])\n","y = np.array([4,5])\n","\n","x + y  # you will see this give an error when you run it, because the shapes of x and y don't match."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TuThKQqPJJql"},"source":["One exception to the above rule is given by operations between a tensor and a scalar."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"SibDY5-CJJql"},"source":["# Exponentiation\n","c = b**2\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lzQQQ5zaJJql"},"source":["# Logical\n","a < 35"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo4o6ei0JJqm"},"source":["NumPy provides standard mathematical functions, such as `sin`, `cos`, and `exp`. They operate **element-wise** on an array, producing an array as output."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"27E8HoewJJqm"},"source":["c = 10*np.sin(b)\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAYVfG1TJJqm"},"source":["---\n","#### Exercise - Sigmoid function\n","\n","The *sigmoid* is a non-linear function that plays a central role in Logistic Regression and Neural Networks. Mathematically, it is defined as \n","\n","$${\\rm sigmoid}(x) = \\frac{1}{1+e^{-x}}.$$ \n","\n","![Sigmoid.png](attachment:Sigmoid.png)\n","\n","**Instructions:** Build a function that returns the sigmoid of a Numpy array `x`. Note that the sigmoid must be computed on every element of `x`.\n","\n","$${\\rm sigmoid}({\\rm x}) = \\begin{bmatrix}\n","    {\\rm sigmoid}(x_1)  \\\\\n","    {\\rm sigmoid}(x_2)  \\\\\n","    \\vdots  \\\\\n","    {\\rm sigmoid}(x_N)  \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","    \\dfrac{1}{1+e^{-x_1}}  \\\\\n","    \\dfrac{1}{1+e^{-x_2}}  \\\\\n","    \\vdots  \\\\\n","    \\dfrac{1}{1+e^{-x_N}}  \\\\\n","\\end{bmatrix} $$"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"04c7a806d3060b58b88706089e07cea8","grade":false,"grade_id":"cell-002abf7f1c2d7e56","locked":false,"schema_version":1,"solution":true},"id":"rrO70J9nJJqm"},"source":["def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    s = None\n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"AHEtf_gUJJqm"},"source":["x = np.array([1, 2, 3])\n","s = sigmoid(x)\n","\n","print('Sigmoid([1,2,3]) = ' + str(s))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"taHCrROKJJqm"},"source":["**Expected Output**: \n","<table>\n","    <tr> \n","        <td> **sigmoid([1,2,3])**</td> \n","        <td> [ 0.73105858,  0.88079708,  0.95257413] </td> \n","    </tr>\n","</table> "]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"83cc19ec018aa0d251153f2fc657a1f0","grade":true,"grade_id":"cell-51e8870090a58894","locked":true,"points":1,"schema_version":1,"solution":false},"id":"HXorFY1GJJqn"},"source":["s.shape == (3,1)\n","np.testing.assert_almost_equal(s, [0.73105858, 0.88079708, 0.95257413])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtFK4Ly4JJqn"},"source":["### Reduction\n","\n","Some operations act on all the elements in a tensor, returning a scalar."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7PKVC0JnJJqn"},"source":["a = np.floor(10*np.random.random((2,3)))\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"x7XWRxpCJJqn"},"source":["# sum of all the elements\n","a.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"H-BFLuZyJJqn"},"source":["# minimum of all the elements\n","a.min()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bHHVkAfbJJqn"},"source":["# cumulative sum of all the elements (in C-style order)\n","a.cumsum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"38LtAwsYJJqo"},"source":["By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the `axis` parameter, you can apply an operation along the specified axis of a multidimensional array (matrices and ND tensors), placing the results in a return array."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"0qVUxR8xJJqo"},"source":["# sum of each column\n","a.sum(axis=0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"0yL69LPDJJqo"},"source":["# minimum of each row\n","a.min(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"tOhFeQ1QJJqo"},"source":["# cumulative sum along each row\n","a.cumsum(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTzuSN56JJqo"},"source":["Note that many reduction operations change the dimensions of the input array: e.g., a matrix becomes a vector. You can keep the dimensions of the input array by setting the `keepdims` parameter to `True`."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"0tGnqwITJJqo"},"source":["b = a.min(axis=1)\n","\n","print(b)\n","print()\n","print(\"Shape: \" + str(b.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Qa6y3IYuJJqp"},"source":["c = a.min(axis=1, keepdims=True)\n","\n","print(c)\n","print()\n","print(\"Shape: \" + str(c.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBSbhRXIJJqp"},"source":["---\n","\n","#### Exercise - Row normalization\n","\n","A common technique we use in Machine Learning is to normalize the data. Here, by normalization we mean dividing each row vector of x by its norm. For example, if $$x = \n","\\begin{bmatrix}\n","    0 & 3 & 4 \\\\\n","    2 & 6 & 4 \\\\\n","\\end{bmatrix}\\tag{3}$$ then $${\\sf row\\_norms}(x) = \\begin{bmatrix}\n","    5 \\\\\n","    \\sqrt{56} \\\\\n","\\end{bmatrix}\\tag{4} $$and        $$ {\\sf x\\_normalized} = \\frac{x}{{\\sf row\\_norms}(x)} = \\begin{bmatrix}\n","    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n","    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n","\\end{bmatrix}\\tag{5}$$ \n","\n","Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in the next section.\n","\n","\n","**Instruction**: Implement `normalizeRows()` to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (length = 1)."]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"47c8dab19f78806ca3ad9d6f6bfff3f7","grade":false,"grade_id":"cell-6e17c32538fa6204","locked":false,"schema_version":1,"solution":true},"id":"jMHwVGIPJJqp"},"source":["def normalizeRows(x):\n","    \"\"\"\n","    Implement a function that normalizes each row of the matrix x (to have unit length).\n","    \n","    Argument:\n","    x -- A numpy matrix of shape (n, m)\n","    \n","    Returns:\n","    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    # Compute the norm of the rows of x. Use np.linalg.norm(..., ord = ..., axis = ..., keepdims = True)\n","    norms = 0\n","    \n","    # Divide x by its norms.\n","    x_norm = 0\n","    \n","    ### END CODE HERE ###\n","    \n","    return x_norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"UtbKkYrZJJqp"},"source":["x = np.array([[0, 3, 4],\n","              [1, 6, 4]])\n","x_norm = normalizeRows(x)\n","\n","print(\"normalizeRows(x):\\n\" + str(x_norm))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNry7CuZJJqp"},"source":["**Expected Output**: \n","\n","<table style=\"width:60%\">\n","\n","     <tr> \n","       <td> **normalizeRows(x)** </td> \n","       <td> [[ 0.          0.6         0.8       ]\n","             [0.13736056  0.82416338  0.54944226]]\n","       </td> \n","     </tr>\n","    \n","   \n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"ed97b192d97eaab6e950450f6d54108b","grade":true,"grade_id":"cell-1422cdd02c62abff","locked":true,"points":1,"schema_version":1,"solution":false},"id":"SrC9j861JJqp"},"source":["assert x.shape == x_norm.shape\n","np.testing.assert_almost_equal(x_norm, [[0., 0.6, 0.8],[0.13736, 0.82416, 0.54944]], decimal=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y9HvXz3CJJqp"},"source":["In `normalizeRows()`, you can try to print the shapes of `x_norm` and `x`, and then rerun the assessment. You'll find out that they have different shapes. This is normal given that `x_norm` takes the norm of each row of `x`. So `x_norm` has the same number of rows but only 1 column. So how did it work when you divided `x` by `x_norm` ? This is called broadcasting and we'll talk about it now! "]},{"cell_type":"markdown","metadata":{"id":"NG3TZluNJJqq"},"source":["### Broadcasting\n","\n","Numpy operations usually involve a pair of arrays, which are combined on an element-by-element basis to produce the result. In the simplest case, the two arrays must have exactly the same shape. However, this constraint can be relaxed when the smaller array can be \"broadcasted\" across the larger array so that they have compatible shapes. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"o2HDRtJYJJqq"},"source":["a = np.array([1.0, 2.0, 3.0])\n","b = 2.0\n","a * b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_Zg6u30JJqq"},"source":["We can think of the scalar `b` being **stretched** during the arithmetic operation into an array with the same shape as the vector `a`. NumPy is smart enough to use the original scalar value without actually making copies, so that broadcasting operations are memory and computationally efficient.\n","\n","![scalar_broadcast.gif](attachment:scalar_broadcast.gif)\n","\n","\n","More generally, when operating on two arrays, Numpy compares their shapes according to the following rules:\n","\n","- **First rule.** If the input arrays do not have the same number of dimensions, a **1** is repeatedly prepended to the shapes of the smaller array until it has the same number of dimensions as the bigger array.\n","\n","- **Second rule.** Arrays with size **1** along a particular dimension are *stretched* to match the size of the array with the largest shape along that dimension.\n","\n","After application of these rules, the sizes of all arrays must match, otherwise an exception is thrown (indicating that the arrays have incompatible shapes). \n","\n","![vector_broadcast.gif](attachment:vector_broadcast.gif)\n","\n","Here are examples of shapes that broadcast:\n","```\n","A      (2d array):  5 x 4\n","B      (1d array):      1\n","Result (2d array):  5 x 4\n","\n","A      (2d array):  5 x 4\n","B      (1d array):      4\n","Result (2d array):  5 x 4\n","\n","A      (3d array):  15 x 3 x 5\n","B      (3d array):  15 x 1 x 5\n","Result (3d array):  15 x 3 x 5\n","\n","A      (3d array):  15 x 3 x 5\n","B      (2d array):       3 x 5\n","Result (3d array):  15 x 3 x 5\n","\n","A      (3d array):  15 x 3 x 5\n","B      (2d array):       3 x 1\n","Result (3d array):  15 x 3 x 5\n","\n","A      (4d array):  8 x 1 x 6 x 1\n","B      (3d array):      7 x 1 x 5\n","Result (4d array):  8 x 7 x 6 x 5\n","```"]},{"cell_type":"markdown","metadata":{"id":"EtnWOP9GJJqq"},"source":["The following example shows an outer addition operation of two 1-d arrays:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9HiSUZtYJJqq"},"source":["a = np.array([0.0, 10.0, 20.0, 30.0])\n","b = np.array([0.0, 1.0, 2.0])\n","\n","a[:, None] + b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjxKo0nUJJqq"},"source":["The `None` index operator inserts a new axis into `a`, making it a `4x1` matrix. Then, it is combined with `b`, which has shape `(3,)`, yielding a `4x3` array.\n","\n","\n","![outer_broadcast.gif](attachment:outer_broadcast.gif)"]},{"cell_type":"markdown","metadata":{"id":"jzygBwCLJJqr"},"source":["Here are examples of shapes that **do not** broadcast:\n","```\n","A      (1d array):  3\n","B      (1d array):  4\n","\n","A      (2d array):      2 x 1\n","B      (3d array):  8 x 4 x 3\n","```\n","\n","The following example shows an incompatible operation: "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"X8aEtBpSJJqr"},"source":["a = np.arange(12).reshape(4,3)\n","b = np.ones(4)\n","a + b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MzGHtAN5JJqr"},"source":["When the trailing dimensions of the arrays are unequal, broadcasting fails because it is impossible to align the values in the rows of the 1st array with the elements of the 2nd arrays for element-by-element addition.\n","\n","![bad_broadcast.gif](attachment:bad_broadcast.gif)"]},{"cell_type":"markdown","metadata":{"id":"ZFl7Fp4jJJqr"},"source":["---\n","\n","#### Exercise - Softmax function\n","\n","The *softmax* is a normalizing function used when your algorithm needs to classify two or more classes. Mathematically, it is defined as follows.\n","\n","- For a vector:\n","\n","$${\\sf softmax}({\\rm x}) = {\\sf softmax}\\left(\\begin{bmatrix}\n","    x_1  &\n","    x_2 &\n","    ...  &\n","    x_N  \n","\\end{bmatrix}\\right) = \\begin{bmatrix}\n","     \\dfrac{e^{x_1}}{\\sum_{j=1}^N e^{x_j}}  &&\n","    \\dfrac{e^{x_2}}{\\sum_{j=1}^N e^{x_j}}  &&\n","    \\dots  &&\n","    \\dfrac{e^{x_N}}{\\sum_{j=1}^N e^{x_j}} \n","\\end{bmatrix} $$\n","\n","- For a matrix:\n","\n","$${\\sf softmax}({\\rm X}) = {\\sf softmax}\\left(\\begin{bmatrix}\n","    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n","    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n","\\end{bmatrix}\\right) = \\begin{bmatrix}\n","    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n","    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n","\\end{bmatrix} = \\begin{pmatrix}\n","    {\\sf softmax}\\text{(first row)}  \\\\\n","    {\\sf softmax}\\text{(second row)} \\\\\n","    ...  \\\\\n","    {\\sf softmax}\\text{(last row)} \\\\\n","\\end{pmatrix} $$\n","\n","**Instructions:** Implement the function `softmax()` in Numpy."]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"9b5e2cb1e7ac8d9c2c6e88abd5919a4a","grade":false,"grade_id":"cell-5c1477e62b442d94","locked":false,"schema_version":1,"solution":true},"id":"o3lA_FCJJJqr"},"source":["def softmax(x):\n","    \"\"\"Calculates the softmax for each row of the input x.\n","\n","    Your code should work for a row vector and also for matrices of shape (n, m).\n","\n","    Argument:\n","    x -- A numpy matrix of shape (n,m)\n","\n","    Returns:\n","    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    \n","    # Compute the exponential of every element in x. Use np.exp(...).\n","    x_exp = None\n","\n","    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = ..., keepdims = True).\n","    x_sum = None\n","    \n","    # Compute softmax by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n","    s = None\n","\n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"1_sIh386JJqr"},"source":["x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0]])\n","y = softmax(x)\n","\n","print(\"softmax(x):\\n\" + str(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sXlPZm4JJqs"},"source":["**Expected Output**:\n","\n","<table style=\"width:60%\">\n","\n","     <tr> \n","       <td> **softmax(x)** </td> \n","       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n","    1.21052389e-04]\n"," [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n","    8.01252314e-04]]</td> \n","     </tr>\n","</table>\n"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"3ed2cdb5a25fa28a67acf295b3bd6c16","grade":true,"grade_id":"cell-791f67f998ea7464","locked":true,"points":1,"schema_version":1,"solution":false},"id":"k4lht3AnJJqs"},"source":["assert x.shape == y.shape\n","np.testing.assert_almost_equal(y, \n","    [[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04, 1.21052389e-04],\n","     [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04, 8.01252314e-04]], \n","    decimal=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHDE7xp5JJqs"},"source":["**Note**: If you print the shapes of `x_exp`, `x_sum` and `s` above and rerun the assessment cell, you will see that `x_sum` is of shape (2,1) while `x_exp` and `s` are of shape (2,5). **`x_exp/x_sum`** works due to python broadcasting."]},{"cell_type":"markdown","metadata":{"id":"NnMUMjz9JJqs"},"source":["## 3. Vectorization\n","\n","In machine learning, you deal with very large datasets. Hence, a slow function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. Vectorization can be roughly divided in two classes:\n","1. the problem you're trying to solve is inherently vectorizable and only requires a few numpy tricks to make it faster,\n","2. you fundamentally have to rethink your problem in order to make it vectorizable.\n","\n","Luckly, most of machine learning algorithms fall in the first category. In the following, you will see a couple of useful vectorization tricks."]},{"cell_type":"markdown","metadata":{"id":"ZPKyDSf9JJqs"},"source":["### Scalar product\n","\n","Let us consider the following problem:\n"," - Given two vectors `x` and `y` of the same size, we want to compute the sum of `x[i]*y[i]` for all indices `i`. \n","\n","One simple and obvious solution is to write the following function."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3v6844gDJJqs"},"source":["def compute_dot_python(x, y):\n","    result = 0\n","    \n","    for i in range(len(x)):\n","        result += x[i] * y[i]\n","        \n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_zuRsLRJJqt"},"source":["However, this implementation requires a loop, which is known to be slow in Python."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"gCZc_UWQJJqt"},"source":["import time\n","\n","x = np.random.randn(1000)\n","\n","tic = time.process_time()\n","compute_dot_python(x,x)\n","toc = time.process_time()\n","\n","print (\"Computation time (python code) = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"vc-EFAoYJJqt"},"source":["import timeit\n","\n","TEST_CODE = ''' \n","x = np.random.randn(10000)\n","y = compute_dot_python(x,x)'''\n","\n","SETUP_CODE = ''' \n","from __main__ import compute_dot_python \n","import numpy as np'''\n","\n","sec = timeit.timeit(TEST_CODE, setup=SETUP_CODE, number=100) / 100\n","\n","print (\"Computation time (python code) =\", 1000*sec, \"ms\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zx8uSrygJJqt"},"source":["How to vectorize our problem? It's simple: by removing the **unnecessary loop.** \n","\n","If you remember your linear algebra course, you may have identified the expression `result += x[i] * y[i]` as a scalar product. Mathematically, the scalar product between two (column) vectors $x\\in\\mathbb{R}^N$ and $y\\in\\mathbb{R}^N$ is\n","\n","$$\n","x^\\top y = [x_1,\\dots,x_N]\\begin{bmatrix}y_1\\\\\\vdots\\\\y_N\\end{bmatrix} = x_1 y_1 + \\dots + x_N y_N = \\sum_{n=1}^N x_n y_n\n","$$\n","\n","Numpy provides the function `np.dot()` to do exactly this. \n","\n","**Remark:** `np.dot()` is different from `np.multiply()` and the `*` operator, which perform an element-wise multiplication."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lmYJmaj0JJqt"},"source":["import timeit\n","\n","TEST_CODE = ''' \n","x = np.random.randn(10000)\n","y = np.dot(x, x)'''\n","\n","SETUP_CODE = ''' \n","from __main__ import compute_dot_python \n","import numpy as np'''\n","\n","sec = timeit.timeit(TEST_CODE, setup=SETUP_CODE, number=100) / 100\n","\n","print (\"Computation time (numpy function) =\", 1000*sec, \"ms\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAl_zcR6JJqu"},"source":["As you have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors, the difference in running time is even larger!"]},{"cell_type":"markdown","metadata":{"id":"P4kIowwjJJqu"},"source":["---\n","#### Exercise - Logistic model\n","\n","The logistic model is used in Machine Learning to perform binary classification. Mathematically, it is defined as \n","\n","$$\n","f_{\\rm w}({\\rm x}) = {\\sf sigmoid}({\\rm w}^\\top {\\rm x}) = {\\sf sigmoid}(w_0 + w_1 x_1 + \\dots + w_Q x_Q)\n","$$\n","\n","where \n","\n","$$\n","{\\rm w} = \\begin{bmatrix}w_0\\\\w_1\\\\\\vdots\\\\w_Q\\end{bmatrix} \\in \\mathbb{R}^{Q+1}\n","\\qquad\\qquad\n","{\\rm x} = \\begin{bmatrix}1\\\\x_1\\\\\\vdots\\\\x_Q\\end{bmatrix} \\in \\mathbb{R}^{Q+1}\n","$$\n","\n","with the convention $x_0=1$.\n","\n","**Instructions**: Implement the logistic model using `np.dot()`. The function takes the vectors `w` and `x` as inputs. Note that such vectors don't have the same length. You can get around this by using one of following tricks:\n","1. Insert **1** at the beginning of `x`, and multiply it with `w`.\n","2. Slice `w` by removing its first element `w[0]`, multiply the remaining elements `w[1], ..., w[Q]` with `x`, and then add `w[0]` to the result."]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"0845d0eda66165d8f6df7a0d0b818a77","grade":false,"grade_id":"cell-c60c11b9d1d22f6d","locked":false,"schema_version":1,"solution":true},"id":"MLpgO8n2JJqu"},"source":["def logistic_prediction_v0(w, x):\n","    \"\"\"Calculates the logistic prediction of the input x.\n","     \n","    Arguments:\n","    w -- A numpy vector of shape (Q+1,)\n","    x -- A numpy vector of shape (Q  ,)\n","\n","    Returns:\n","    s -- A scalar equal to the logistic prediction of x\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    # Compute w0 + w1*x1 + ... + wQ*xQ. Use np.dot(...).\n","    w_dot_x = None\n","    \n","    # Evaluate the sigmoid function in w_dot_x. Use your function implemented before.\n","    s = None\n","    \n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"eja6GdflJJqu"},"source":["x = np.array([3, 2, 7])\n","w = np.array([-3, 2, 0.5, -1])\n","\n","s = logistic_prediction_v0(w,x)\n","\n","print(\"logistic_prediction_v0(w,x) = \" + str(s))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7eCo1CcfJJqu"},"source":["**Expected Output**:\n","\n","<table style=\"width:50%\">\n","    <tr>\n","        <td>  ** logistic prediction (v0) **  </td>\n","      <td> 0.04742587 </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"16bf97d0908e5974f64dc06c752bda3a","grade":true,"grade_id":"cell-e3eb2cbc79333edd","locked":true,"points":1,"schema_version":1,"solution":false},"id":"ewuBFaJIJJqu"},"source":["assert s.shape == ()\n","np.testing.assert_almost_equal(s, 0.04742587, decimal=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Gu-np2YJJqv"},"source":["### Matrix-vector product\n","\n","The function `np.dot()` is also capable of performing matrix-vector or vector-matrix products. Assume that `A` is a matrix of size $N\\times K$, `b` is a vector of size $K$, and `c` is a vector of size $N$:\n","\n","$$\n","A = \n","\\begin{bmatrix}\n","a_{1,1} & \\dots & a_{1,K}\\\\\n","\\vdots &  & \\vdots\\\\\n","a_{N,1} & \\dots & a_{N,K}\\\\\n","\\end{bmatrix}\n","\\qquad\\qquad\n","b = \\begin{bmatrix}b_1\\\\\\vdots\\\\b_K\\end{bmatrix}\n","\\qquad\\qquad\n","c = \\begin{bmatrix}c_1\\\\\\vdots\\\\c_N\\end{bmatrix}\n","$$\n","\n","Mathematically, the exact operation performed by `np.dot()` depends on the order of inputs:\n","\n","- **`np.dot(A,b)`** multiplies `b` with the *rows* of `A`\n","\n","$$\n","Ab = \\left[ \\sum_{k=1}^K a_{n,k} b_k \\right]_{1\\le n\\le N}\n","$$\n","\n","- **`np.dot(c,A)`** multiplies `c` with the *columns* of `A` \n","\n","$$\n","c^\\top A = \\left[ \\sum_{n=1}^N a_{n,k} c_n \\right]_{1\\le k\\le K}\n","$$\n","\n","In both cases, the result is a vector (1-D array)."]},{"cell_type":"markdown","metadata":{"id":"zB0VnW4VJJqv"},"source":["---\n","\n","#### Exercise - Prediction on batch\n","\n","In logistic regression, you normally need to compute the prediction for many samples ${\\rm x}^{(1)}, \\dots, {\\rm x}^{(N)}$. This computation can be easily vectorized.\n","\n","- Stack the samples as rows of a matrix $X$ with size $N\\times Q$:\n","\n","$$\n","X = \\begin{bmatrix}\n","\\_\\!\\_\\; {{\\rm x}^{(1)}}^\\top \\_\\!\\_ \\\\\n","\\vdots\\\\\n","\\_\\!\\_\\; {{\\rm x}^{(N)}}^\\top \\_\\!\\_ \\\\\n","\\end{bmatrix}.\n","$$\n","\n","- Compute the products $z_n = {\\rm w}^\\top{\\rm x}^{(n)}$ for every index $n$. This is equivalent to multiplying $X$ by ${\\rm w}$:\n","\n","$$ {\\rm z} = X{\\rm w} =\n","\\begin{bmatrix}\n","{\\rm w}^\\top{\\rm x}^{(1)}\\\\\n","\\vdots\\\\\n","{\\rm w}^\\top{\\rm x}^{(N)}\n","\\end{bmatrix}.$$\n","\n","\n","- Compute the logistic function of $z_n$ for every index $n$:\n","\n","$$\n","{\\sf sigmoid}({\\rm z}) =\n","\\begin{bmatrix}\n","{\\sf sigmoid}(z_1)\\\\\n","\\vdots\\\\\n","{\\sf sigmoid}(z_N)\\\\\n","\\end{bmatrix}.\n","$$\n","\n","\n","\n","**Instructions**: Modify your implementation of `logistic_prediction()` so that it takes a vector `w` and a matrix `X` as inputs. However, pay attention to the fact that `X` is a matrix of size $N\\times Q$, while `w` is a vector of length $Q+1$. As before, you can either slice `w`, or insert a column of **1** in front of `X`.\n","\n"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"6dfefcb7206554962496acd9227ade99","grade":false,"grade_id":"cell-0421400d422d341f","locked":false,"schema_version":1,"solution":true},"id":"j68yK4j-JJqv"},"source":["def logistic_prediction(w, x):\n","    \"\"\"Calculates the logistic prediction for each row of the input x.\n","     \n","    Arguments:\n","    w -- A numpy vector of shape (Q+1,)\n","    x -- A numpy matrix of shape ( N, Q)\n","\n","    Returns:\n","    s -- A numpy vector of shape (N,)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    # Compute w0 + w1*x1 + ... + wQ*xQ. Use np.dot(...).\n","    w_dot_x = None\n","\n","    # Evaluate the sigmoid function in x_dot_w. Use your function implemented before.\n","    s = None\n","    \n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8OfCkhEcJJqv"},"source":["X = np.array([[4, -5,  2],\n","              [5,  3, -3],\n","              [5,  2,  1],\n","              [8,  6,  9],\n","              [5, -6,  2]])\n","\n","y = logistic_prediction(w, X)\n","\n","print(\"y.shape: \" + str(y.shape))\n","print(\"logistic_prediction(w,X): \" + str(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7W2xbLrNJJqv"},"source":["**Expected Output**:\n","\n","<table style=\"width:50%\">\n","    <tr>\n","      <td>  ** y.shape **  </td>\n","      <td> (5,) </td>\n","    </tr>\n","        <tr>\n","      <td>  ** logistic prediction **  </td>\n","      <td> [0.62245933 0.99998987 0.99908895 0.99908895 0.88079708] </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"9dcf38e6b9c3005a8b76bb8e8936424c","grade":true,"grade_id":"cell-574784464c041353","locked":true,"points":1,"schema_version":1,"solution":false},"id":"6SqBYa9LJJqv"},"source":["assert y.shape == (5,)\n","np.testing.assert_almost_equal(y, [0.62245933, 0.99998987, 0.99908895, 0.99908895, 0.88079708], decimal=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjjytNw-JJqw"},"source":["### Matrix multiplication\n","\n","When both inputs are matrices (2-D arrays), the function `np.dot()` performs a matrix multiplication.\n","\n","- Given two matrices $A\\in\\mathbb{R}^{M\\times N}$ and $B\\in\\mathbb{R}^{N\\times K}$, their product is equal to the scalar product between the rows of $A$ and the columns of $B$:\n","\n","$$\n","AB = \n","\\begin{bmatrix}\n","\\_\\!\\_\\; {\\rm a}_1^\\top \\,\\_\\!\\_ \\\\\n","\\vdots\\\\\n","\\_\\!\\_\\; {\\rm a}_M^\\top \\,\\_\\!\\_ \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","| & & |\\\\[-1em]\n","{\\rm b}_1 & \\dots & {\\rm b}_K\\\\\n","| & & |\\\\\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","{\\rm a}_1^\\top{\\rm b}_1 & \\dots & {\\rm a}_1^\\top{\\rm b}_K\\\\\n","\\vdots & & \\vdots\\\\\n","{\\rm a}_M^\\top{\\rm b}_1 & \\dots & {\\rm a}_M^\\top{\\rm b}_K\n","\\end{bmatrix}.\n","$$\n","\n","\n","- In general, the inputs of `np.dot()` can be scalars (0-D arrays), vectors (1-D arrays), matrices (2-D arrays), or tensors (N-D arrays). \n","\n","\n","- The exact meaning of `np.dot()` depends on the shape of its inputs."]},{"cell_type":"markdown","metadata":{"id":"WA0Uj44wJJqw"},"source":["#### Exercise - Multiclass prediction\n","\n","In multiclass logistic regression, you have many parameter vectors ${\\rm w}_1, \\dots, {\\rm w}_K$ (one for each class), and you need to compute the prediction for many samples ${\\rm x}^{(1)}, \\dots, {\\rm x}^{(N)}$. This computation can be also vectorized. \n","\n","- Store the parameter vectors as columns of a matrix $W$ with size $(Q+1)\\times K$.\n","\n","$$\n","W = \\begin{bmatrix}\n","| & & |\\\\[-1em]\n","{\\rm w}_1 & \\dots & {\\rm w}_K\\\\\n","| & & |\\\\\n","\\end{bmatrix}.\n","$$\n","\n","\n","- Compute the products $z_{n,k} = {{\\rm x}^{(n)}}^\\top {\\rm w}_k$ for every index $k$ and $n$. This is equivalent to multiplying $X$ by $W$:\n","\n","$$ Z = XW = \n","\\begin{bmatrix}\n","{{\\rm x}^{(1)}}^\\top{\\rm w}_1 & \\dots & {{\\rm x}^{(1)}}^\\top{\\rm w}_K\\\\\n","\\vdots & & \\vdots\\\\\n","{{\\rm x}^{(N)}}^\\top{\\rm w}_1 & \\dots & {{\\rm x}^{(N)}}^\\top{\\rm w}_K\n","\\end{bmatrix}.\n","$$\n","\n","\n","- Compute the softmax on every row ${\\rm z}_n$ of $Z$:\n","\n","$$\n","{\\sf softmax}(Z) = \n","\\begin{bmatrix}\n","{\\sf softmax}({\\rm z}_1)\\\\\n","\\vdots\\\\\n","{\\sf softmax}({\\rm z}_N)\n","\\end{bmatrix}.\n","$$\n","\n","**Instructions**: Implement the softmax prediction using the function `np.dot()`. However, pay attention to the fact that `X` is a matrix of size $N\\times Q$, while `W` is a matrix of size $(Q+1)\\times K$. As before, you can either insert a column of **1** in front of `X`, or slice `W`."]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"38fa3c3500b67a3ff07e3780947fa927","grade":false,"grade_id":"cell-28d12f7fa132eae1","locked":false,"schema_version":1,"solution":true},"id":"NZrFOZSLJJqw"},"source":["def softmax_prediction(w, x):\n","    \"\"\"Calculates the logistic prediction for each row of the input X.\n","     \n","    Arguments:\n","    w -- A numpy matrix of shape (Q+1,K)\n","    x -- A numpy matrix of shape ( N, Q)\n","\n","    Returns:\n","    s -- A numpy vector of shape (N,K)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    # Compute w0 + w1*x1 + ... + wQ*xQ. Use np.dot(...).\n","    w_dot_x = None\n","    \n","    # Evaluate the softmax function in x_dot_w. Use your function implemented before.\n","    s = None\n","    \n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"D-z5XnQDJJqw"},"source":["W = np.array(\n","    [[-3,  2,  4,  5,  6,  7,  2], \n","     [ 2, -1,  4, -2,  4,  2, -7], \n","     [ 1,  2, -4,  6, -1,  5,  6], \n","     [-1,  1,  6,  7,  3, -3,  5]\n","    ]\n",")\n","\n","y = softmax_prediction(W, X)\n","\n","print(\"y.shape: \" + str(y.shape))\n","print(\"\\nsoftmax_prediction(w,x):\\n\\n\" + str(y))\n","print(\"\\nargmax: \" + str( np.argmax(y, axis=1) ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OBhiocBJJqw"},"source":["**Expected Output**:\n","\n","<table style=\"width:80%\">\n","    <tr>\n","      <td>  ** y.shape **  </td>\n","      <td> (5,7) </td>\n","    </tr>\n","        <tr>\n","      <td>  ** softmax prediction **  </td>\n","      <td> [[  3.53262855e-24   1.18506486e-27   9.99999994e-01   1.46248622e-31\n","    5.60279641e-09   2.93748210e-30   2.74878499e-43]\n"," [  6.91440011e-13   1.56288219e-18   3.87399763e-21   5.24288566e-22\n","    1.87952882e-12   1.00000000e+00   1.46248623e-31]\n"," [  5.30303054e-09   1.31448985e-11   6.37744724e-03   2.13939521e-06\n","    9.46497093e-01   4.71233155e-02   2.00196538e-19]\n"," [  1.33361482e-34   1.97925988e-32   2.78946809e-10   1.00000000e+00\n","    2.54366565e-13   1.18506486e-27   3.22134029e-27]\n"," [  3.22134029e-27   1.97925988e-32   1.00000000e+00   1.64581143e-38\n","    2.78946809e-10   4.90609473e-35   2.08428284e-52]] </td>\n","    </tr>\n","    <tr>\n","      <td>  ** argmax **  </td>\n","      <td> [2 5 4 3 2] </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"a0a33e2e7a3c16c2398f343009e7558a","grade":true,"grade_id":"cell-6bcca7ecd5d096ba","locked":true,"points":1,"schema_version":1,"solution":false},"id":"8b622vOFJJqw"},"source":["assert y.shape == (5,7)\n","np.testing.assert_almost_equal(y, \n","    [[3.53262855e-24, 1.18506486e-27, 9.99999994e-01, 1.46248622e-31, 5.60279641e-09, 2.93748210e-30, 2.74878499e-43],\n","     [6.91440011e-13, 1.56288219e-18, 3.87399763e-21, 5.24288566e-22, 1.87952882e-12, 1.00000000e+00, 1.46248623e-31],\n","     [5.30303054e-09, 1.31448985e-11, 6.37744724e-03, 2.13939521e-06, 9.46497093e-01, 4.71233155e-02, 2.00196538e-19],\n","     [1.33361482e-34, 1.97925988e-32, 2.78946809e-10, 1.00000000e+00, 2.54366565e-13, 1.18506486e-27, 3.22134029e-27],\n","     [3.22134029e-27, 1.97925988e-32, 1.00000000e+00, 1.64581143e-38, 2.78946809e-10, 4.90609473e-35, 2.08428284e-52]],\n","    decimal=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5mLy32JJJqw"},"source":["---\n","\n","#### Exercise - Linear regression\n","\n","Linear regression is a machine learning technique that aims at predicting a continuous output by using a parametric linear model:\n","\n","$$ f_{\\rm w}({\\rm x}) = {\\rm w}^\\top {\\rm x} = w_0 + w_1 x_1 + \\dots + w_Q x_Q. $$\n","\n","Hereabove, ${\\rm w}$ is a parameter vector to be learned from a set of input-output pairs $\\{({\\rm x}^{(n)}, y^{(n)})\\}$. Specifically, the learning consists of finding the vector ${\\rm w}$ such that the prediction $f_{\\rm w}({\\rm x}^{(n)})$ is close to the true output $y^{(n)}$ for every index $n$. Mathematically, this is equivalent to minimize the following loss function:\n","\n","$$ J({\\rm w}) = \\sum_{n=1}^N \\big(y^{(n)} - {\\rm w}^\\top {\\rm x}^{(n)}\\big)^2. $$\n","\n","The loss is used to evaluate the performance of the model. The bigger the loss is, the more different the predictions $f_{\\rm w}({\\rm x}^{(n)})$ are from the true values $y^{(n)}$.\n","\n","\n","**Instruction**: Implement a function that computes the loss (a scalar) given three inputs: \n"," - `w` - the parameter vector, \n"," - `X` - the matrix storing all the inputs ${\\rm x}^{(n)}$, \n"," - `y` - the vector with all the outputs $y^{(n)}$.\n","\n","Don't use loops. The implementation must be fully vectorized!"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"nbgrader":{"checksum":"32735fb5186795da65cb0c5d69aef143","grade":false,"grade_id":"cell-4906d03247148bd4","locked":false,"schema_version":1,"solution":true},"id":"IRgp8D6SJJqx"},"source":["def linear_regression_loss(w, X, y):\n","    \"\"\"Calculates the loss used in linear regression.\n","     \n","    Arguments:\n","    w -- Numpy vector of shape (Q+1,)\n","    X -- Numpy matrix of shape (N, Q)\n","    y -- Numpy vector of shape (N,)\n","\n","    Returns:\n","    s -- Scalar\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    # Compute w0 + w1*x1 + ... + wQ*xQ. Use np.dot(...).\n","    w_dot_x = None\n","\n","    # Compute the squared distance between x_dot_w and y. Use np.sum()\n","    J = None\n","    \n","    ### END CODE HERE ###\n","    \n","    return J"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"RcmcteYDJJqx"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4Gyd6atcJJqx"},"source":["w = np.array([-3, 2, 0.5, -1])\n","X = np.array([[4, -5,  2],\n","              [5,  3, -3],\n","              [5,  2,  1],\n","              [8,  6,  9],\n","              [5, -6,  2]])\n","y = np.array([6, -3, 0.5, 1, 0.])\n","\n","J = linear_regression_loss(w, X, y)\n","\n","print(\"Shape: \" + str(J.shape))\n","print(\"linear_regression_loss(w,X,y): \" + str(J))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBBFu46cJJqx"},"source":["<table style=\"width:50%\">\n","    <tr>\n","      <td>  ** Shape **  </td>\n","      <td> ( ) </td>\n","    </tr>\n","        <tr>\n","      <td>  ** Loss **  </td>\n","      <td> 322.75 </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"collapsed":true,"deletable":false,"editable":false,"nbgrader":{"checksum":"04d157c1f22501a122ca9e4d9a39b941","grade":true,"grade_id":"cell-6084311d2f055b49","locked":true,"points":1,"schema_version":1,"solution":false},"id":"b4LiHgHdJJqx"},"source":["assert J.shape == ()\n","np.testing.assert_almost_equal(J, 322.75, decimal=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rH_LQ0igJJqx"},"source":["## 4. Conclusion\n","\n","Congratulations! You now have a pretty good understanding of Numpy, and have implemented a few functions that you will be using in deep learning. We hope this little warm-up tutorial helps you in the future assignments, which will be more exciting and interesting! \n","\n","**What have you learned:**\n","\n","- How to use Numpy arrays and its efficient built-in functions\n","- The concept of Broadcasting\n","- How to vectorize code\n","\n","For deeper insights, additional information can be found here:\n","\n","- [Python](http://docs.python.org/tut/)\n","\n","- [Numpy](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n","\n","- [Broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n","\n","- [Vectorization](http://www.labri.fr/perso/nrougier/from-python-to-numpy/)"]}]}